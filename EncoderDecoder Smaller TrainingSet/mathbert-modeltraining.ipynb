{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b944f4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:37.025229Z",
     "iopub.status.busy": "2024-01-09T14:00:37.024847Z",
     "iopub.status.idle": "2024-01-09T14:00:37.044569Z",
     "shell.execute_reply": "2024-01-09T14:00:37.043787Z"
    },
    "papermill": {
     "duration": 0.028106,
     "end_time": "2024-01-09T14:00:37.046492",
     "exception": false,
     "start_time": "2024-01-09T14:00:37.018386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class Vocabulary:\n",
    "    PAD_token = 0     # Used for padding short sentences\n",
    "    SOS_token = 1     # Start-of-sentence token\n",
    "    EOS_token = 2     # End-of-sentence token\n",
    "    def __init__(self, name, sentence_trim=50):\n",
    "        self.PAD_token = 0     # Used for padding short sentences\n",
    "        self.SOS_token = 1     # Start-of-sentence token\n",
    "        self.EOS_token = 2     # End-of-sentence token\n",
    "        self.NEW_WORD = 3\n",
    "        self.START_NUMBER = 4\n",
    "        self.END_NUMBER = 5\n",
    "        self.name = name\n",
    "        self.word2index = { \n",
    "            \"new_word\" : self.NEW_WORD,\n",
    "            \"PAD\" : self.PAD_token,\n",
    "            \"SOS\" : self.SOS_token,\n",
    "            \"EOS\" : self.EOS_token,\n",
    "            \"<num>\" : self.START_NUMBER,\n",
    "            \"</num>\" : self.END_NUMBER\n",
    "        }\n",
    "        self.word2count = {\n",
    "            \"new_word\" : 1,\n",
    "            \"PAD\" : 1,\n",
    "            \"SOS\" : 1,\n",
    "            \"EOS\" : 1,\n",
    "            \"<num>\" : 1,\n",
    "            \"</num>\" : 1\n",
    "        }\n",
    "        self.index2word = {\n",
    "            self.PAD_token: \"PAD\",\n",
    "            self.SOS_token: \"SOS\",\n",
    "            self.EOS_token: \"EOS\",\n",
    "            self.NEW_WORD : \"new_word\",\n",
    "            self.START_NUMBER: \"<num>\",\n",
    "            self.END_NUMBER: \"</num>\"\n",
    "        }\n",
    "        self.num_words = 6\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "        self.sentence_number = []\n",
    "        self.sentence_trim = sentence_trim\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        sentence = sentence.replace('|',' | ').replace(',',' , ').replace(':',' : ').replace('\\n','').strip()\n",
    "        numbers = re.findall('([-\\d\\.]+)',sentence)\n",
    "        formattedSentence = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word in numbers:\n",
    "                word = [\"<num>\"] + [ c for c in word ] + [\"</num>\"]\n",
    "                for c in word:\n",
    "                    formattedSentence.append(c)\n",
    "            else:\n",
    "                formattedSentence.append(word.lower())\n",
    "        for word in formattedSentence:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentence_number.append(sentence_len)\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word.lower()]\n",
    "\n",
    "    def sentence_to_index(self, sentence, padding=False):\n",
    "        sentence = sentence.replace('|',' | ').replace(',',' , ').replace(':',' : ').replace('\\n','').strip()\n",
    "        numbers = re.findall('([-\\d\\.]+)',sentence)\n",
    "        formattedSentence = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word in numbers:\n",
    "                    word = [\"<num>\"] + [ c for c in word ] + [\"</num>\"]\n",
    "                    for c in word:\n",
    "                            formattedSentence.append(c)\n",
    "            else:\n",
    "                    formattedSentence.append(word.lower())\n",
    "        array = []\n",
    "        array.append(self.SOS_token)\n",
    "        for tok in formattedSentence:\n",
    "            try:\n",
    "                    array.append(self.to_index(tok))\n",
    "            except:\n",
    "                    #self.add_word(tok.text)\n",
    "                    #array.append(self.to_index(tok.text))\n",
    "                    array.append(self.to_index(\"new_word\"))\n",
    "        array.append(self.EOS_token)\n",
    "        if padding:\n",
    "            for i in range(len(array),self.sentence_trim):\n",
    "                    array.append(self.PAD_token)\n",
    "            return array[:self.sentence_trim]\n",
    "        else:\n",
    "            return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6132f",
   "metadata": {
    "papermill": {
     "duration": 0.005364,
     "end_time": "2024-01-09T14:00:37.057396",
     "exception": false,
     "start_time": "2024-01-09T14:00:37.052032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5134b45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:37.069435Z",
     "iopub.status.busy": "2024-01-09T14:00:37.069171Z",
     "iopub.status.idle": "2024-01-09T14:00:40.810264Z",
     "shell.execute_reply": "2024-01-09T14:00:40.809449Z"
    },
    "papermill": {
     "duration": 3.74961,
     "end_time": "2024-01-09T14:00:40.812532",
     "exception": false,
     "start_time": "2024-01-09T14:00:37.062922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358a9806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:40.825305Z",
     "iopub.status.busy": "2024-01-09T14:00:40.824914Z",
     "iopub.status.idle": "2024-01-09T14:00:43.962250Z",
     "shell.execute_reply": "2024-01-09T14:00:43.961466Z"
    },
    "papermill": {
     "duration": 3.146191,
     "end_time": "2024-01-09T14:00:43.964548",
     "exception": false,
     "start_time": "2024-01-09T14:00:40.818357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_to_file(objeto, nome_arquivo):\n",
    "    with open(nome_arquivo, 'wb') as output:\n",
    "        pickle.dump(objeto, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_file(nome_arquivo):\n",
    "    with open(nome_arquivo, 'rb') as input:\n",
    "        objeto = pickle.load(input)\n",
    "    return objeto\n",
    "\n",
    "sub_path = './'\n",
    "\n",
    "vocab = load_file(sub_path+'vocab.pkl')\n",
    "\n",
    "src = load_file(sub_path+'src.pkl')\n",
    "trg = load_file(sub_path+'trg.pkl')\n",
    "\n",
    "srcVal = load_file(sub_path+'srcVal.pkl')\n",
    "trgVal = load_file(sub_path+'trgVal.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8e1ec",
   "metadata": {
    "papermill": {
     "duration": 0.005144,
     "end_time": "2024-01-09T14:00:43.975868",
     "exception": false,
     "start_time": "2024-01-09T14:00:43.970724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62be70a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:43.988373Z",
     "iopub.status.busy": "2024-01-09T14:00:43.988043Z",
     "iopub.status.idle": "2024-01-09T14:00:44.008931Z",
     "shell.execute_reply": "2024-01-09T14:00:44.008039Z"
    },
    "papermill": {
     "duration": 0.029415,
     "end_time": "2024-01-09T14:00:44.010857",
     "exception": false,
     "start_time": "2024-01-09T14:00:43.981442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Seq2Seq Network using Transformer\n",
    "# ---------------------------------\n",
    "#\n",
    "# Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
    "# need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
    "# paper for solving machine translation tasks.\n",
    "# Below, we will create a Seq2Seq network that uses Transformer. The network\n",
    "# consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
    "# into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
    "# encodings to provide position information of input tokens to the model. The second part is the\n",
    "# actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model.\n",
    "# Finally, the output of the Transformer model is passed through linear layer\n",
    "# that gives unnormalized probabilities for each token in the target language.\n",
    "#\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = -1, 0, 1, 2\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# During training, we need a subsequent word mask that will prevent the model from looking into\n",
    "# the future words when making predictions. We will also need masks to hide\n",
    "# source and target padding tokens. Below, let's define a function that will take care of both.\n",
    "#\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5476ff",
   "metadata": {
    "papermill": {
     "duration": 0.005204,
     "end_time": "2024-01-09T14:00:44.021346",
     "exception": false,
     "start_time": "2024-01-09T14:00:44.016142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a68cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:44.159482Z",
     "iopub.status.busy": "2024-01-09T14:00:44.159201Z",
     "iopub.status.idle": "2024-01-09T14:00:44.236818Z",
     "shell.execute_reply": "2024-01-09T14:00:44.235898Z"
    },
    "papermill": {
     "duration": 0.086219,
     "end_time": "2024-01-09T14:00:44.238730",
     "exception": false,
     "start_time": "2024-01-09T14:00:44.152511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb916813",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Let's now define the parameters of our model and instantiate the same. Below, we also\n",
    "# define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = vocab.num_words+1\n",
    "TGT_VOCAB_SIZE = vocab.num_words+1\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "model = transformer\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30630709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,856,983 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e64d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len, start_symbol=1):\n",
    "    #print(src.shape)\n",
    "    src = (src.reshape(max_len,1))\n",
    "    #print(src.shape)\n",
    "    num_tokens = len(src)\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(device)\n",
    "    memory = model.encode(src, src_mask).to(device)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == 2: # 2 == EOS\n",
    "          break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f60aeec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:44.769508Z",
     "iopub.status.busy": "2024-01-09T14:00:44.769239Z",
     "iopub.status.idle": "2024-01-09T14:00:44.781396Z",
     "shell.execute_reply": "2024-01-09T14:00:44.780525Z"
    },
    "papermill": {
     "duration": 0.020842,
     "end_time": "2024-01-09T14:00:44.783303",
     "exception": false,
     "start_time": "2024-01-09T14:00:44.762461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "BATCHSIZE = 128\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train_bert(model, optimizer, criterion, src, trg):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    it = 0\n",
    "    for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "        it += 1\n",
    "        # As transformações matriciais do treinamento existem porque o tgt tem que prever x+1 ou seja, se a entrada é x, a saída é x+1.\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src[:,i].to(device), trg[:-1,i].to(device))\n",
    "        output = model(\n",
    "            src[:,i].to(device), trg[:-1,i].to(device), src_mask,\n",
    "            tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), trg[1:,i].to(device).view(-1)) # Loss da saída de X+1\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE\n",
    "\n",
    "def evaluate_bert(model, criterion, src, trg):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src[:,i].to(device), trg[:-1,i].to(device))\n",
    "            output = model(\n",
    "                src[:,i].to(device), trg[:-1,i].to(device), src_mask,\n",
    "                tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask\n",
    "            )\n",
    "            #loss = criterion(output.argmax(2).reshape(-1,1)[:,0], trg[i,:].to(device).reshape(-1,1)[:,0])\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), trg[1:,i].to(device).view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ffe21ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:44.836345Z",
     "iopub.status.busy": "2024-01-09T14:00:44.835927Z",
     "iopub.status.idle": "2024-01-09T14:00:44.839805Z",
     "shell.execute_reply": "2024-01-09T14:00:44.838969Z"
    },
    "papermill": {
     "duration": 0.012353,
     "end_time": "2024-01-09T14:00:44.841725",
     "exception": false,
     "start_time": "2024-01-09T14:00:44.829372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7da50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = src.T\n",
    "trg = trg.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8666d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "srcVal = srcVal.T\n",
    "trgVal = trgVal.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7f4d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 19914]),\n",
       " torch.Size([60, 19914]),\n",
       " torch.Size([60, 4968]),\n",
       " torch.Size([60, 4968]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape, srcVal.shape, trgVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d036c705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asw = greedy_decode(model, src[:,2].to(device), 60, 1)\n",
    "' '.join( vocab.index2word[i] for i in asw.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71eb770c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T14:00:44.854403Z",
     "iopub.status.busy": "2024-01-09T14:00:44.854158Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-01-09T14:00:44.847576",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully!\n",
      "Start training 0\n",
      "Validating... 0\n",
      "Epoch: 01 | Time: 2m 12s\n",
      "\tTrain Loss: 2.349 | Train PPL:  10.474\n",
      "\t Val. Loss: 0.188 |    Val. PPL:   1.206\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS would you be willing to calculate this function :  <num> 3 </num> + <num> 1 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS it's my belief that the answer could be :  <num> 1 0 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 1\n",
      "Validating... 1\n",
      "Epoch: 02 | Time: 2m 41s\n",
      "\tTrain Loss: 0.751 | Train PPL:   2.119\n",
      "\t Val. Loss: 0.165 |    Val. PPL:   1.179\n",
      "Start training 2\n",
      "Validating... 2\n",
      "Epoch: 03 | Time: 2m 54s\n",
      "\tTrain Loss: 0.678 | Train PPL:   1.969\n",
      "\t Val. Loss: 0.156 |    Val. PPL:   1.169\n",
      "Start training 3\n",
      "Validating... 3\n",
      "Epoch: 04 | Time: 2m 59s\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.892\n",
      "\t Val. Loss: 0.146 |    Val. PPL:   1.157\n",
      "Start training 4\n",
      "Validating... 4\n",
      "Epoch: 05 | Time: 2m 51s\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.138 |    Val. PPL:   1.148\n",
      "Start training 5\n",
      "Validating... 5\n",
      "Epoch: 06 | Time: 2m 49s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.131 |    Val. PPL:   1.140\n",
      "Start training 6\n",
      "Validating... 6\n",
      "Epoch: 07 | Time: 2m 53s\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.127 |    Val. PPL:   1.136\n",
      "Start training 7\n",
      "Validating... 7\n",
      "Epoch: 08 | Time: 2m 54s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.700\n",
      "\t Val. Loss: 0.123 |    Val. PPL:   1.131\n",
      "Start training 8\n",
      "Validating... 8\n",
      "Epoch: 09 | Time: 2m 49s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.119 |    Val. PPL:   1.126\n",
      "Start training 9\n",
      "Validating... 9\n",
      "Epoch: 10 | Time: 2m 48s\n",
      "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
      "\t Val. Loss: 0.116 |    Val. PPL:   1.123\n",
      "Start training 10\n",
      "Validating... 10\n",
      "Epoch: 11 | Time: 2m 51s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "\t Val. Loss: 0.115 |    Val. PPL:   1.122\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS could you compute the following formula :  <num> 7 </num> + <num> 4 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i have a feeling the solution is :  <num> 1 1 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 11\n",
      "Validating... 11\n",
      "Epoch: 12 | Time: 2m 48s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.114 |    Val. PPL:   1.120\n",
      "Start training 12\n",
      "Validating... 12\n",
      "Epoch: 13 | Time: 2m 52s\n",
      "\tTrain Loss: 0.460 | Train PPL:   1.584\n",
      "\t Val. Loss: 0.113 |    Val. PPL:   1.120\n",
      "Start training 13\n",
      "Validating... 13\n",
      "Epoch: 14 | Time: 2m 49s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.111 |    Val. PPL:   1.117\n",
      "Start training 14\n",
      "Validating... 14\n",
      "Epoch: 15 | Time: 2m 51s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.111 |    Val. PPL:   1.118\n",
      "Start training 15\n",
      "Validating... 15\n",
      "Epoch: 16 | Time: 2m 51s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.110 |    Val. PPL:   1.117\n",
      "Start training 16\n",
      "Validating... 16\n",
      "Epoch: 17 | Time: 2m 49s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.110 |    Val. PPL:   1.116\n",
      "Start training 17\n",
      "Validating... 17\n",
      "Epoch: 18 | Time: 2m 52s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.107 |    Val. PPL:   1.113\n",
      "Start training 18\n",
      "Validating... 18\n",
      "Epoch: 19 | Time: 3m 26s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.105 |    Val. PPL:   1.111\n",
      "Start training 19\n",
      "Validating... 19\n",
      "Epoch: 20 | Time: 4m 11s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
      "\t Val. Loss: 0.104 |    Val. PPL:   1.110\n",
      "Start training 20\n",
      "Validating... 20\n",
      "Epoch: 21 | Time: 3m 40s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.104 |    Val. PPL:   1.110\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS hi there ,  would you figure out the following function :  <num> 4 </num> <num> - </num> <num> 9 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i have a feeling the solution is :  <num> - 5 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 21\n",
      "Validating... 21\n",
      "Epoch: 22 | Time: 3m 16s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.109\n",
      "Start training 22\n",
      "Validating... 22\n",
      "Epoch: 23 | Time: 3m 8s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.109\n",
      "Start training 23\n",
      "Validating... 23\n",
      "Epoch: 24 | Time: 3m 21s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.102 |    Val. PPL:   1.107\n",
      "Start training 24\n",
      "Validating... 24\n",
      "Epoch: 25 | Time: 4m 8s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.102 |    Val. PPL:   1.107\n",
      "Start training 25\n",
      "Validating... 25\n",
      "Epoch: 26 | Time: 3m 55s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.102 |    Val. PPL:   1.107\n",
      "Start training 26\n",
      "Validating... 26\n",
      "Epoch: 27 | Time: 3m 29s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.102 |    Val. PPL:   1.107\n",
      "Start training 27\n",
      "Validating... 27\n",
      "Epoch: 28 | Time: 3m 35s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.108\n",
      "Start training 28\n",
      "Validating... 28\n",
      "Epoch: 29 | Time: 3m 27s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.108\n",
      "Start training 29\n",
      "Validating... 29\n",
      "Epoch: 30 | Time: 3m 23s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.109\n",
      "Start training 30\n",
      "Validating... 30\n",
      "Epoch: 31 | Time: 3m 17s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.103 |    Val. PPL:   1.108\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS can you analyze this function :  ( <num> 7 </num> + ( <num> 3 </num> + <num> 1 </num> ) ) EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i believe the answer might be :  <num> 1 1 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 31\n",
      "Validating... 31\n",
      "Epoch: 32 | Time: 2m 59s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.104 |    Val. PPL:   1.110\n",
      "Start training 32\n",
      "Validating... 32\n",
      "Epoch: 33 | Time: 2m 58s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.104 |    Val. PPL:   1.110\n",
      "Start training 33\n",
      "Validating... 33\n",
      "Epoch: 34 | Time: 2m 53s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.104 |    Val. PPL:   1.110\n",
      "Start training 34\n",
      "Validating... 34\n",
      "Epoch: 35 | Time: 2m 55s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.105 |    Val. PPL:   1.110\n",
      "Start training 35\n",
      "Validating... 35\n",
      "Epoch: 36 | Time: 2m 57s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.106 |    Val. PPL:   1.112\n",
      "Start training 36\n",
      "Validating... 36\n",
      "Epoch: 37 | Time: 2m 54s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.107 |    Val. PPL:   1.113\n",
      "Start training 37\n",
      "Validating... 37\n",
      "Epoch: 38 | Time: 2m 53s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.107 |    Val. PPL:   1.113\n",
      "Start training 38\n",
      "Validating... 38\n",
      "Epoch: 39 | Time: 2m 54s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.108 |    Val. PPL:   1.114\n",
      "Start training 39\n",
      "Validating... 39\n",
      "Epoch: 40 | Time: 2m 50s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.109 |    Val. PPL:   1.115\n",
      "Start training 40\n",
      "Validating... 40\n",
      "Epoch: 41 | Time: 2m 54s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.110 |    Val. PPL:   1.116\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS are you capable of calculating this function :  ( <num> 1 </num> <num> - </num> <num> 8 </num> ) + <num> 4 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i have a feeling the solution is :  <num> - 3 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 41\n",
      "Validating... 41\n",
      "Epoch: 42 | Time: 2m 52s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.110 |    Val. PPL:   1.116\n",
      "Start training 42\n",
      "Validating... 42\n",
      "Epoch: 43 | Time: 2m 52s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.111 |    Val. PPL:   1.118\n",
      "Start training 43\n",
      "Validating... 43\n",
      "Epoch: 44 | Time: 2m 48s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.112 |    Val. PPL:   1.119\n",
      "Start training 44\n",
      "Validating... 44\n",
      "Epoch: 45 | Time: 2m 51s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.113 |    Val. PPL:   1.120\n",
      "Start training 45\n",
      "Validating... 45\n",
      "Epoch: 46 | Time: 2m 50s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.115 |    Val. PPL:   1.122\n",
      "Start training 46\n",
      "Validating... 46\n",
      "Epoch: 47 | Time: 2m 53s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.115 |    Val. PPL:   1.122\n",
      "Start training 47\n",
      "Validating... 47\n",
      "Epoch: 48 | Time: 2m 51s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.117 |    Val. PPL:   1.124\n",
      "Start training 48\n",
      "Validating... 48\n",
      "Epoch: 49 | Time: 2m 52s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.118 |    Val. PPL:   1.125\n",
      "Start training 49\n",
      "Validating... 49\n",
      "Epoch: 50 | Time: 2m 48s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.119 |    Val. PPL:   1.126\n",
      "Start training 50\n",
      "Validating... 50\n",
      "Epoch: 51 | Time: 2m 52s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.120 |    Val. PPL:   1.128\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS salutations ,  would you determine the subsequent function :  <num> 3 </num> + <num> 3 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i would hazard a guess that the solution might be :  <num> 6 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 51\n",
      "Validating... 51\n",
      "Epoch: 52 | Time: 2m 48s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.121 |    Val. PPL:   1.128\n",
      "Start training 52\n",
      "Validating... 52\n",
      "Epoch: 53 | Time: 2m 39s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.122 |    Val. PPL:   1.130\n",
      "Start training 53\n",
      "Validating... 53\n",
      "Epoch: 54 | Time: 2m 39s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.124 |    Val. PPL:   1.132\n",
      "Start training 54\n",
      "Validating... 54\n",
      "Epoch: 55 | Time: 2m 38s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.125 |    Val. PPL:   1.133\n",
      "Start training 55\n",
      "Validating... 55\n",
      "Epoch: 56 | Time: 2m 39s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.126 |    Val. PPL:   1.135\n",
      "Start training 56\n",
      "Validating... 56\n",
      "Epoch: 57 | Time: 2m 40s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.127 |    Val. PPL:   1.135\n",
      "Start training 57\n",
      "Validating... 57\n",
      "Epoch: 58 | Time: 2m 51s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.130 |    Val. PPL:   1.139\n",
      "Start training 58\n",
      "Validating... 58\n",
      "Epoch: 59 | Time: 2m 52s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.131 |    Val. PPL:   1.140\n",
      "Start training 59\n",
      "Validating... 59\n",
      "Epoch: 60 | Time: 2m 53s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.133 |    Val. PPL:   1.142\n",
      "Start training 60\n",
      "Validating... 60\n",
      "Epoch: 61 | Time: 2m 51s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.134 |    Val. PPL:   1.143\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS salutations ,  could you evaluate the succeeding equation :  ( <num> 5 </num> + ( <num> 1 0 </num> * <num> 3 </num> ) ) EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS in my opinion ,  the solution might be :  <num> 2 5 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 61\n",
      "Validating... 61\n",
      "Epoch: 62 | Time: 2m 49s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.135 |    Val. PPL:   1.145\n",
      "Start training 62\n",
      "Validating... 62\n",
      "Epoch: 63 | Time: 2m 48s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.137 |    Val. PPL:   1.146\n",
      "Start training 63\n",
      "Validating... 63\n",
      "Epoch: 64 | Time: 2m 47s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.137 |    Val. PPL:   1.147\n",
      "Start training 64\n",
      "Validating... 64\n",
      "Epoch: 65 | Time: 2m 45s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.139 |    Val. PPL:   1.150\n",
      "Start training 65\n",
      "Validating... 65\n",
      "Epoch: 66 | Time: 2m 46s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.141 |    Val. PPL:   1.152\n",
      "Start training 66\n",
      "Validating... 66\n",
      "Epoch: 67 | Time: 2m 47s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.143 |    Val. PPL:   1.154\n",
      "Start training 67\n",
      "Validating... 67\n",
      "Epoch: 68 | Time: 2m 50s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.144 |    Val. PPL:   1.155\n",
      "Start training 68\n",
      "Validating... 68\n",
      "Epoch: 69 | Time: 2m 46s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.145 |    Val. PPL:   1.156\n",
      "Start training 69\n",
      "Validating... 69\n",
      "Epoch: 70 | Time: 2m 48s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.149 |    Val. PPL:   1.160\n",
      "Start training 70\n",
      "Validating... 70\n",
      "Epoch: 71 | Time: 2m 46s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.150 |    Val. PPL:   1.162\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS would you be able to derive this equation :  ( <num> 7 </num> <num> - </num> ( <num> 3 </num> * <num> 5 </num> ) ) EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i have a feeling the solution is :  <num> - 1 2 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 71\n",
      "Validating... 71\n",
      "Epoch: 72 | Time: 2m 49s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.151 |    Val. PPL:   1.163\n",
      "Start training 72\n",
      "Validating... 72\n",
      "Epoch: 73 | Time: 2m 47s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.154 |    Val. PPL:   1.167\n",
      "Start training 73\n",
      "Validating... 73\n",
      "Epoch: 74 | Time: 2m 46s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.157 |    Val. PPL:   1.170\n",
      "Start training 74\n",
      "Validating... 74\n",
      "Epoch: 75 | Time: 2m 56s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.158 |    Val. PPL:   1.172\n",
      "Start training 75\n",
      "Validating... 75\n",
      "Epoch: 76 | Time: 2m 54s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.160 |    Val. PPL:   1.174\n",
      "Start training 76\n",
      "Validating... 76\n",
      "Epoch: 77 | Time: 2m 55s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.163 |    Val. PPL:   1.177\n",
      "Start training 77\n",
      "Validating... 77\n",
      "Epoch: 78 | Time: 2m 54s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.164 |    Val. PPL:   1.179\n",
      "Start training 78\n",
      "Validating... 78\n",
      "Epoch: 79 | Time: 2m 53s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.168 |    Val. PPL:   1.183\n",
      "Start training 79\n",
      "Validating... 79\n",
      "Epoch: 80 | Time: 2m 50s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.170 |    Val. PPL:   1.185\n",
      "Start training 80\n",
      "Validating... 80\n",
      "Epoch: 81 | Time: 2m 52s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.172 |    Val. PPL:   1.188\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS can you solve this subsequent mathematical function :  <num> 5 </num> + <num> 1 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS it's conceivable that the solution is :  <num> 6 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 81\n",
      "Validating... 81\n",
      "Epoch: 82 | Time: 2m 49s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.174 |    Val. PPL:   1.190\n",
      "Start training 82\n",
      "Validating... 82\n",
      "Epoch: 83 | Time: 2m 50s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.176 |    Val. PPL:   1.193\n",
      "Start training 83\n",
      "Validating... 83\n",
      "Epoch: 84 | Time: 2m 49s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.178 |    Val. PPL:   1.195\n",
      "Start training 84\n",
      "Validating... 84\n",
      "Epoch: 85 | Time: 2m 52s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.182 |    Val. PPL:   1.200\n",
      "Start training 85\n",
      "Validating... 85\n",
      "Epoch: 86 | Time: 2m 48s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.182 |    Val. PPL:   1.200\n",
      "Start training 86\n",
      "Validating... 86\n",
      "Epoch: 87 | Time: 2m 47s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.185 |    Val. PPL:   1.203\n",
      "Start training 87\n",
      "Validating... 87\n",
      "Epoch: 88 | Time: 2m 51s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.187 |    Val. PPL:   1.205\n",
      "Start training 88\n",
      "Validating... 88\n",
      "Epoch: 89 | Time: 2m 50s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.190 |    Val. PPL:   1.209\n",
      "Start training 89\n",
      "Validating... 89\n",
      "Epoch: 90 | Time: 2m 50s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.190 |    Val. PPL:   1.209\n",
      "Start training 90\n",
      "Validating... 90\n",
      "Epoch: 91 | Time: 2m 50s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.193 |    Val. PPL:   1.213\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS are you able to solve this function :  <num> 6 </num> + <num> 1 </num> <num> - </num> <num> 1 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS from where i stand ,  the solution could be :  <num> 4 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 91\n",
      "Validating... 91\n",
      "Epoch: 92 | Time: 2m 52s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.194 |    Val. PPL:   1.214\n",
      "Start training 92\n",
      "Validating... 92\n",
      "Epoch: 93 | Time: 2m 49s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.199 |    Val. PPL:   1.220\n",
      "Start training 93\n",
      "Validating... 93\n",
      "Epoch: 94 | Time: 2m 51s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.201 |    Val. PPL:   1.223\n",
      "Start training 94\n",
      "Validating... 94\n",
      "Epoch: 95 | Time: 2m 49s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.202 |    Val. PPL:   1.224\n",
      "Start training 95\n",
      "Validating... 95\n",
      "Epoch: 96 | Time: 2m 42s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.204 |    Val. PPL:   1.227\n",
      "Start training 96\n",
      "Validating... 96\n",
      "Epoch: 97 | Time: 2m 38s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.207 |    Val. PPL:   1.230\n",
      "Start training 97\n",
      "Validating... 97\n",
      "Epoch: 98 | Time: 2m 36s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.208 |    Val. PPL:   1.231\n",
      "Start training 98\n",
      "Validating... 98\n",
      "Epoch: 99 | Time: 2m 37s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.210 |    Val. PPL:   1.234\n",
      "Start training 99\n",
      "Validating... 99\n",
      "Epoch: 100 | Time: 2m 39s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.216 |    Val. PPL:   1.241\n",
      "Start training 100\n",
      "Validating... 100\n",
      "Epoch: 101 | Time: 2m 40s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.215 |    Val. PPL:   1.240\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tSOS could you analyze the following function :  <num> 2 </num> + <num> 7 </num> + <num> 9 </num> EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "\n",
      "\t Answer:\n",
      "\tSOS i am of the opinion that the response could be :  <num> 1 9 </num> EOS\n",
      "\n",
      "\n",
      "\n",
      "Start training 101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/4166424851.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start training'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validating...'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrcVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrgVal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/1510953424.py\u001b[0m in \u001b[0;36mtrain_bert\u001b[1;34m(model, optimizer, criterion, src, trg)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m#torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vormenesse\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "path = './'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "try: \n",
    "    model.load_state_dict(torch.load(path+'math-bert-model.pt'))\n",
    "    print('Model Loaded Successfully!')\n",
    "except:\n",
    "    print('No model loaded, starting training from scratch.')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('Start training',epoch)\n",
    "    train_loss = train_bert(model, optimizer, criterion, src, trg)\n",
    "    print('Validating...',epoch)\n",
    "    valid_loss = evaluate_bert(model, criterion, srcVal, trgVal)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), path+'math-bert-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "        textFile.write(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\\n')\n",
    "        textFile.write(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\\n')\n",
    "        textFile.write(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}\\n')\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            rnd = np.random.randint(1,100)\n",
    "            o = greedy_decode(model, srcVal[:,rnd-1:rnd].to(device),60, 1)\n",
    "            query = ' '.join( vocab.index2word[i] for i in srcVal[:,rnd-1:rnd].squeeze().cpu().numpy())\n",
    "            answer = ' '.join( vocab.index2word[i] for i in o.squeeze().cpu().numpy())\n",
    "        print(f'Testing:\\n')\n",
    "        print(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "        print(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "        print(f'\\n')\n",
    "        with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "            textFile.write(f'Testing:\\n')\n",
    "            textFile.write(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "            textFile.write(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "            textFile.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9088b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4229902,
     "sourceId": 7340043,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-09T14:00:32.399834",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
