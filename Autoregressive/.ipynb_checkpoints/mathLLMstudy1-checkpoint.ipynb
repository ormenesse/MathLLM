{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, head_count):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.head_count = head_count # attention heads\n",
    "        # create linear layers for query, key and value projections for each head\n",
    "        self.query_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(embed_size, embed_size, bias=False)\n",
    "                for _ in range(head_count)\n",
    "            ]\n",
    "        )\n",
    "        self.key_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(embed_size, embed_size, bias=False)\n",
    "                for _ in range(head_count)\n",
    "            ]\n",
    "        )\n",
    "        self.value_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(embed_size, embed_size, bias=False)\n",
    "                for _ in range(head_count)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(head_count * embed_size, embed_size) # final layer to combine head outputs\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        batch_size, token_count = embeddings[:2]\n",
    "        qkvs = torch.zeros(self.head_count, 3, batch_size, token_count,\n",
    "            self.embed_size\n",
    "        ).to(device)\n",
    "\n",
    "        # looping over heads to compute query, key and value projections\n",
    "        for i in range(self.head_count):\n",
    "            qkvs[i, 0] = self.query_layers[i](embeddings)\n",
    "            qkvs[i, 1] = self.key_layers[i](embeddings)\n",
    "            qkvs[i, 2] = self.value_layers[i](embeddings)\n",
    "        \n",
    "        # computing energy term for each head, batch, adn pair of tokens\n",
    "        energy = torch.zeros(self.head_count, batch_size, token_count, token_count).to(device)\n",
    "        # create mask with false on below the diagonal and true above the diagonal\n",
    "        mask = torch.triu(torch.ones((token_count, token_count), diagonal=1)).bool().to(device)\n",
    "\n",
    "        for h in range(self.head_count): \n",
    "            for b in range(batch_size):\n",
    "                for i in range(token_count):\n",
    "                    for j in range(token_count):\n",
    "                        energy[h, b, i, j] = torch.dot(qkvs[h, 0, b, i], qkvs[h, i, b, j]) # energy for each word with respect to every other word.\n",
    "                energy[h, b] = energy[h, b].masked_fill(mask, float('-inf')) # making sure only preceding energy is calculated\n",
    "        \n",
    "        # attention scores\n",
    "        attention = torch.nn.functional.softmax(energy, dim=3) # masked attention\n",
    "\n",
    "        # compute weighted sum of alues for each head and token\n",
    "        out = torch.zeros(batch_size, token_count, self.head_count, self.embed_size).to(device)\n",
    "        for h in range(self.head_count):\n",
    "            for b in range(batch_size):\n",
    "                for i in range(token_count):\n",
    "                    for j in range(token_count):\n",
    "                        out[b, i, h] +=(attention[h,b, i, j] * qkvs[h, 2, b, j]) # weighted sum of value vector for each token\n",
    "        \n",
    "        out = out.reshape(batch_size, token_count, self.head_count * self.embed_size)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # ResNet architecture, we can stack many transformers in a block to increase performance\n",
    "    def __init__(self, embed_size, head_count):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, head_count) # self attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # feed forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, embed_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        attention = self.attention(embeddings)\n",
    "        #\n",
    "        out = self.norm1(attention + embeddings)\n",
    "        out = attention + self.feed_forward(out)\n",
    "        out = self.norm2(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, head_count):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size # size of your word embeddings\n",
    "        self.vocab_size = vocab_size # size of your vocab\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, head_count)\n",
    "                for _ in range(num_layers)\n",
    "            ] \n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size) # final linear layer to produce logits\n",
    "\n",
    "    def forward(self, input_tokens, mask=None):\n",
    "        batch_size, token_count = input_tokens.shape[:2]\n",
    "        out = self.word_embedding(input_tokens) # word embedding\n",
    "\n",
    "        # positional enconding\n",
    "        positions = torch.arange(0, token_count).expand(batch_size, token_count)\n",
    "        positional_encoding = self.positional_encoding(positions, self.embed_size)\n",
    "        out += positional_encoding.reshape(out.shape)\n",
    "\n",
    "        # pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        # produce logits for the final token in each sequence\n",
    "        out = self.fc_out(out[:, -1, :].reshape(batch_size, self.emb_size)).reshape(batch_size, self.vocab_size)\n",
    "\n",
    "        return torch.nn.funcional.softmax(out, dim=1)\n",
    "\n",
    "    def positional_encoding(self, positions, embed_size):\n",
    "        angle_rads = self.get_angles(\n",
    "            positions.unsqueeze(2).float(),\n",
    "            torch.arange(embed_size)[None, None, :].float().to(device),\n",
    "            embed_size\n",
    "        )\n",
    "        sines = torch.sin(angle_rads[:, :, 0::2])\n",
    "        cosines = torch.cos(angle_rads[:, :, 1::2])\n",
    "        pos_encoding = torch.cat([sines, cosines], dim=1)\n",
    "        pos_encoding = pos_encoding.unsqueeze() # pos_enconding[None, ...]\n",
    "        return pos_encoding\n",
    "\n",
    "    def get_angles(self, pos, i, embed_size):\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / emb_size )\n",
    "        return pos * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Let's now define the parameters of our model and instantiate the same. Below, we also\n",
    "# define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = 151#vocab.num_words+1\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Transformer(VOCAB_SIZE, EMB_SIZE, FFN_HID_DIM, NHEAD)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 285,482,135 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
