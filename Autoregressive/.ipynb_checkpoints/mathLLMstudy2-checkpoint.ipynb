{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pprint\n",
    "import math\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size).to(device)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Autoregressive(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Autoregressive, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_size).to(device)\n",
    "        # positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout=0.1).to(device)\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Embedding input sequence\n",
    "        embedded_seq = self.embedding(input_seq)\n",
    "        embedded_seq = self.positional_encoding(embedded_seq)\n",
    "        # Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embedded_seq = transformer_block(embedded_seq)\n",
    "\n",
    "        # Prediction\n",
    "        output = self.fc(embedded_seq[-1, :, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Let's now define the parameters of our model and instantiate the same. Below, we also\n",
    "# define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = 151#vocab.num_words+1\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Autoregressive(VOCAB_SIZE, EMB_SIZE, FFN_HID_DIM, NHEAD)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "#model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,373,847 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_to_file(objeto, nome_arquivo):\n",
    "    with open(nome_arquivo, 'wb') as output:\n",
    "        pickle.dump(objeto, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_file(nome_arquivo):\n",
    "    with open(nome_arquivo, 'rb') as input:\n",
    "        objeto = pickle.load(input)\n",
    "    return objeto\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "sub_path = './'\n",
    "\n",
    "vocab = load_file(sub_path+'vocab.pkl')\n",
    "\n",
    "src = load_file(sub_path+'src.pkl')\n",
    "trg = load_file(sub_path+'trg.pkl')\n",
    "\n",
    "srcVal = load_file(sub_path+'srcVal.pkl')\n",
    "trgVal = load_file(sub_path+'trgVal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 1341439]),\n",
       " torch.Size([1341439]),\n",
       " torch.Size([60, 335448]),\n",
       " torch.Size([335448]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape, srcVal.shape, trgVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "BATCHSIZE = 128\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, optimizer, criterion, src, trg):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    it = 0\n",
    "    for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "        it += 1\n",
    "        output = model(\n",
    "            src[:,i].to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), trg[i].to(device).view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE\n",
    "\n",
    "def evaluate(model, criterion, src, trg):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "            output = model(\n",
    "                src[:,i].to(device)\n",
    "            )\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), trg[i].to(device).view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, src, maxlen=30):\n",
    "    response = []\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            output = model(src.to(device))\n",
    "            word = output.squeeze().argmax()\n",
    "            response.append(int(word.cpu().numpy()))\n",
    "            src = torch.cat((srcVal[:,rnd-1:rnd].cpu(),torch.tensor([[int(word.cpu().numpy())]])),dim=0).to('cpu')\n",
    "            if word == 2 or len(response) == maxlen:\n",
    "                break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rnd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m o \u001b[38;5;241m=\u001b[39m decode(model, srcVal[:,rnd\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:rnd]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin( vocab\u001b[38;5;241m.\u001b[39mindex2word[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m o))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "rnd = np.random.randint(1,100)\n",
    "o = decode(model, srcVal[:,rnd-1:rnd].to(device),60)\n",
    "print(' '.join( vocab.index2word[i] for i in o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "path = './'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "try: \n",
    "    model.load_state_dict(torch.load(path+'math-bert-model.pt'))\n",
    "    print('Model Loaded Successfully!')\n",
    "except:\n",
    "    print('No model loaded, starting training from scratch.')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('Start training',epoch)\n",
    "    train_loss = train(model, optimizer, criterion, src, trg)\n",
    "    print('Validating...',epoch)\n",
    "    valid_loss = evaluate(model, criterion, srcVal, trgVal)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), path+'math-bert-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "        textFile.write(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\\n')\n",
    "        textFile.write(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\\n')\n",
    "        textFile.write(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}\\n')\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            rnd = np.random.randint(1,100)\n",
    "            o = decode(model, srcVal[:,rnd-1:rnd].to(device),60)\n",
    "            query = ' '.join( vocab.index2word[i] for i in srcVal[:,rnd-1:rnd].squeeze().cpu().numpy())\n",
    "            answer = ' '.join( vocab.index2word[i] for i in o)\n",
    "        print(f'Testing:\\n')\n",
    "        print(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "        print(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "        print(f'\\n')\n",
    "        with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "            textFile.write(f'Testing:\\n')\n",
    "            textFile.write(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "            textFile.write(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "            textFile.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
