{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pprint\n",
    "import math\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size).to(device)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Autoregressive(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Autoregressive, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_size).to(device)\n",
    "        # positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout=0.1).to(device)\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Embedding input sequence\n",
    "        embedded_seq = self.embedding(input_seq)\n",
    "        embedded_seq = self.positional_encoding(embedded_seq)\n",
    "        # Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embedded_seq = transformer_block(embedded_seq)\n",
    "\n",
    "        # Prediction\n",
    "        output = self.fc(embedded_seq[-1, :, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Let's now define the parameters of our model and instantiate the same. Below, we also\n",
    "# define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = 151#vocab.num_words+1\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Autoregressive(VOCAB_SIZE, EMB_SIZE, FFN_HID_DIM, NHEAD)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,373,847 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_to_file(objeto, nome_arquivo):\n",
    "    with open(nome_arquivo, 'wb') as output:\n",
    "        pickle.dump(objeto, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_file(nome_arquivo):\n",
    "    with open(nome_arquivo, 'rb') as input:\n",
    "        objeto = pickle.load(input)\n",
    "    return objeto\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "sub_path = './'\n",
    "\n",
    "vocab = load_file(sub_path+'vocab.pkl')\n",
    "\n",
    "src = load_file(sub_path+'src.pkl')\n",
    "trg = load_file(sub_path+'trg.pkl')\n",
    "\n",
    "srcVal = load_file(sub_path+'srcVal.pkl')\n",
    "trgVal = load_file(sub_path+'trgVal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 1342084]),\n",
       " torch.Size([1342084]),\n",
       " torch.Size([60, 335385]),\n",
       " torch.Size([335385]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape, srcVal.shape, trgVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "BATCHSIZE = 128\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, optimizer, criterion, src, trg):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    it = 0\n",
    "    for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "        it += 1\n",
    "        output = model(\n",
    "            src[:,i].to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), trg[i].to(device).view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE\n",
    "\n",
    "def evaluate(model, criterion, src, trg):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in chunks(np.arange(src.shape[1]), BATCHSIZE):\n",
    "            output = model(\n",
    "                src[:,i].to(device)\n",
    "            )\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), trg[i].to(device).view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return epoch_loss / BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, src, maxlen=30):\n",
    "    response = []\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            output = model(src.to(device))\n",
    "            word = output.squeeze().argmax()\n",
    "            response.append(int(word.cpu().numpy()))\n",
    "            src = torch.cat((srcVal[:,rnd-1:rnd].cpu(),torch.tensor([[int(word.cpu().numpy())]])),dim=0)\n",
    "            if word == 2 or len(response) == maxlen:\n",
    "                break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 make 4 make make formula 4 4 understand <num> 3 formula standpoint 4 understand 4 understand 4 4 4 <num> succeeding 4 make 4 standpoint 3 determine make 4 i 4 impression 3 make 4 understand evaluate out standpoint make 4 hunch 4 standpoint impression make standpoint 4 4 evaluate make 4 succeeding 4 3 4 4 make\n"
     ]
    }
   ],
   "source": [
    "rnd = np.random.randint(1,100)\n",
    "o = decode(model, srcVal[:,rnd-1:rnd].to(device),60)\n",
    "print(' '.join( vocab.index2word[i] for i in o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model loaded, starting training from scratch.\n",
      "Start training 0\n",
      "Validating... 0\n",
      "Epoch: 01 | Time: 76m 51s\n",
      "\tTrain Loss: 39.223 | Train PPL: 108212401616218864.000\n",
      "\t Val. Loss: 7.886 |    Val. PPL: 2660.236\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tPAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD SOS can you evaluate this succeeding mathematical expression :  ( <num> 4 </num> + <num> 7 </num> ) EOS SOS it's my judgment that the solution could be :\n",
      "\n",
      "\t Answer:\n",
      "\t <num> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "\n",
      "Start training 1\n",
      "Validating... 1\n",
      "Epoch: 02 | Time: 80m 22s\n",
      "\tTrain Loss: 30.431 | Train PPL: 16440252434873.045\n",
      "\t Val. Loss: 6.999 |    Val. PPL: 1095.862\n",
      "Start training 2\n",
      "Validating... 2\n",
      "Epoch: 03 | Time: 80m 16s\n",
      "\tTrain Loss: 28.334 | Train PPL: 2018913718625.519\n",
      "\t Val. Loss: 6.707 |    Val. PPL: 817.755\n",
      "Start training 3\n",
      "Validating... 3\n",
      "Epoch: 04 | Time: 82m 17s\n",
      "\tTrain Loss: 27.505 | Train PPL: 881475316235.576\n",
      "\t Val. Loss: 6.606 |    Val. PPL: 739.388\n",
      "Start training 4\n",
      "Validating... 4\n",
      "Epoch: 05 | Time: 82m 42s\n",
      "\tTrain Loss: 26.909 | Train PPL: 485723113273.095\n",
      "\t Val. Loss: 6.465 |    Val. PPL: 642.570\n",
      "Start training 5\n",
      "Validating... 5\n",
      "Epoch: 06 | Time: 81m 22s\n",
      "\tTrain Loss: 26.674 | Train PPL: 383896846795.976\n",
      "\t Val. Loss: 6.507 |    Val. PPL: 670.068\n",
      "Start training 6\n",
      "Validating... 6\n",
      "Epoch: 07 | Time: 82m 14s\n",
      "\tTrain Loss: 26.685 | Train PPL: 388274633454.293\n",
      "\t Val. Loss: 6.517 |    Val. PPL: 676.594\n",
      "Start training 7\n",
      "Validating... 7\n",
      "Epoch: 08 | Time: 82m 26s\n",
      "\tTrain Loss: 26.598 | Train PPL: 355894758501.411\n",
      "\t Val. Loss: 6.435 |    Val. PPL: 623.243\n",
      "Start training 8\n",
      "Validating... 8\n",
      "Epoch: 09 | Time: 81m 43s\n",
      "\tTrain Loss: 26.746 | Train PPL: 412578738697.279\n",
      "\t Val. Loss: 6.420 |    Val. PPL: 613.973\n",
      "Start training 9\n",
      "Validating... 9\n",
      "Epoch: 10 | Time: 539m 39s\n",
      "\tTrain Loss: 26.853 | Train PPL: 459479721121.720\n",
      "\t Val. Loss: 6.483 |    Val. PPL: 653.764\n",
      "Start training 10\n",
      "Validating... 10\n",
      "Epoch: 11 | Time: 69m 53s\n",
      "\tTrain Loss: 27.428 | Train PPL: 816031122272.001\n",
      "\t Val. Loss: 6.464 |    Val. PPL: 641.706\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tPAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD SOS can you solve this subsequent mathematical function :  ( <num> 7 </num> + <num> 5 </num> ) EOS SOS it strikes me\n",
      "\n",
      "\t Answer:\n",
      "\tas possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible as possible that the plausible\n",
      "\n",
      "\n",
      "\n",
      "Start training 11\n",
      "Validating... 11\n",
      "Epoch: 12 | Time: 72m 56s\n",
      "\tTrain Loss: 27.626 | Train PPL: 995292852865.769\n",
      "\t Val. Loss: 6.752 |    Val. PPL: 855.425\n",
      "Start training 12\n",
      "Validating... 12\n",
      "Epoch: 13 | Time: 79m 23s\n",
      "\tTrain Loss: 28.472 | Train PPL: 2318854711875.586\n",
      "\t Val. Loss: 6.546 |    Val. PPL: 696.498\n",
      "Start training 13\n",
      "Validating... 13\n",
      "Epoch: 14 | Time: 81m 18s\n",
      "\tTrain Loss: 28.839 | Train PPL: 3347571039008.751\n",
      "\t Val. Loss: 6.831 |    Val. PPL: 926.460\n",
      "Start training 14\n",
      "Validating... 14\n",
      "Epoch: 15 | Time: 82m 59s\n",
      "\tTrain Loss: 29.551 | Train PPL: 6817785502237.641\n",
      "\t Val. Loss: 6.963 |    Val. PPL: 1056.782\n",
      "Start training 15\n",
      "Validating... 15\n",
      "Epoch: 16 | Time: 82m 48s\n",
      "\tTrain Loss: 29.662 | Train PPL: 7624034108094.428\n",
      "\t Val. Loss: 6.794 |    Val. PPL: 892.208\n",
      "Start training 16\n",
      "Validating... 16\n",
      "Epoch: 17 | Time: 82m 19s\n",
      "\tTrain Loss: 29.700 | Train PPL: 7916863480728.961\n",
      "\t Val. Loss: 7.120 |    Val. PPL: 1236.137\n",
      "Start training 17\n",
      "Validating... 17\n",
      "Epoch: 18 | Time: 79m 36s\n",
      "\tTrain Loss: 30.153 | Train PPL: 12450078825537.885\n",
      "\t Val. Loss: 6.902 |    Val. PPL: 994.114\n",
      "Start training 18\n",
      "Validating... 18\n",
      "Epoch: 19 | Time: 351m 48s\n",
      "\tTrain Loss: 30.694 | Train PPL: 21397287749960.125\n",
      "\t Val. Loss: 6.893 |    Val. PPL: 985.540\n",
      "Start training 19\n",
      "Validating... 19\n",
      "Epoch: 20 | Time: 552m 55s\n",
      "\tTrain Loss: 31.069 | Train PPL: 31118924533144.133\n",
      "\t Val. Loss: 7.289 |    Val. PPL: 1463.774\n",
      "Start training 20\n",
      "Validating... 20\n",
      "Epoch: 21 | Time: 147m 46s\n",
      "\tTrain Loss: 31.296 | Train PPL: 39037020891882.453\n",
      "\t Val. Loss: 7.817 |    Val. PPL: 2483.037\n",
      "Testing:\n",
      "\n",
      "\t Query:\n",
      "\tPAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD SOS hey ,  can you calculate the subsequent mathematical expression :  ( <num> 2 </num> + <num> 2 </num> ) EOS SOS it's my belief\n",
      "\n",
      "\t Answer:\n",
      "\tthat the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is that the answer is\n",
      "\n",
      "\n",
      "\n",
      "Start training 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart training\u001b[39m\u001b[38;5;124m'\u001b[39m,epoch)\n\u001b[0;32m---> 16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train(model, optimizer, criterion, src, trg)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidating...\u001b[39m\u001b[38;5;124m'\u001b[39m,epoch)\n\u001b[1;32m     18\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, criterion, srcVal, trgVal)\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, src, trg)\u001b[0m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 31\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m BATCHSIZE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "path = './'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "try: \n",
    "    model.load_state_dict(torch.load(path+'math-bert-model.pt'))\n",
    "    print('Model Loaded Successfully!')\n",
    "except:\n",
    "    print('No model loaded, starting training from scratch.')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('Start training',epoch)\n",
    "    train_loss = train(model, optimizer, criterion, src, trg)\n",
    "    print('Validating...',epoch)\n",
    "    valid_loss = evaluate(model, criterion, srcVal, trgVal)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), path+'math-bert-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "        textFile.write(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\\n')\n",
    "        textFile.write(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\\n')\n",
    "        textFile.write(f'\\t Val. Loss: {valid_loss:.3f} |    Val. PPL: {math.exp(valid_loss):7.3f}\\n')\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            rnd = np.random.randint(1,100)\n",
    "            o = decode(model, srcVal[:,rnd-1:rnd].to(device),60)\n",
    "            query = ' '.join( vocab.index2word[i] for i in srcVal[:,rnd-1:rnd].squeeze().cpu().numpy())\n",
    "            answer = ' '.join( vocab.index2word[i] for i in o)\n",
    "        print(f'Testing:\\n')\n",
    "        print(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "        print(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "        print(f'\\n')\n",
    "        with open(path+\"modelTrainingOutput.txt\", \"a\") as textFile:\n",
    "            textFile.write(f'Testing:\\n')\n",
    "            textFile.write(f'\\t Query:\\n\\t'+query+'\\n')\n",
    "            textFile.write(f'\\t Answer:\\n\\t'+answer+'\\n')\n",
    "            textFile.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
